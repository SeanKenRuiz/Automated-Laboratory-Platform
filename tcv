from ultralytics import YOLO
import cv2 as cv
import numpy as np
import torch

# Function to calculate IoU (Intersection over Union) between two bounding boxes
def intersection_over_union(bbox, cbox):
    ix1 = np.maximum(bbox[0], cbox[0])
    iy1 = np.maximum(bbox[1], cbox[1])
    ix2 = np.minimum(bbox[2], cbox[2])
    iy2 = np.minimum(bbox[3], cbox[3])

    i_height = np.maximum(iy2 - iy1, 0)
    i_width = np.maximum(ix2 - ix1, 0)

    area_of_intersection = i_height * i_width

    gt_height = bbox[3] - bbox[1]
    gt_width = bbox[2] - bbox[0]

    pd_height = cbox[3] - cbox[1]
    pd_width = cbox[2] - cbox[0]

    area_of_union = (gt_height * gt_width) + (pd_height * pd_width) - area_of_intersection

    if area_of_union == 0:
        return 0.0
    return area_of_intersection / area_of_union

# Convert xywh to xyxy format
def xywh_to_xyxy(x, y, w, h):
    x1 = x - w / 2
    y1 = y - h / 2
    x2 = x + w / 2
    y2 = y + h / 2
    return [x1, y1, x2, y2]

# Function to calculate and display object orientations
def get_and_display_object_orientations(frame, tray_bbox):
    object_orientations = []

    # Convert image to grayscale and then to binary
    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)
    _, bw = cv.threshold(gray, 50, 255, cv.THRESH_BINARY | cv.THRESH_OTSU)

    # Find all contours
    contours, _ = cv.findContours(bw, cv.RETR_LIST, cv.CHAIN_APPROX_SIMPLE)

    for c in contours:
        area = cv.contourArea(c)

        # Ignore contours that are too small or too large
        if area < 3700 or area > 100000:
            continue

        # Get the minimum area rectangle
        rect = cv.minAreaRect(c)
        box = cv.boxPoints(rect)
        box = np.intp(box)

        # Get key parameters of the rotated bounding box
        center = (int(rect[0][0]), int(rect[0][1]))
        width, height = rect[1]
        angle = rect[2]

        if width < height:
            angle = 90 - angle
        else:
            angle = -angle

        contour_box = xywh_to_xyxy(center[0], center[1], width, height)

        # Convert the tray_bbox and contour_box to numpy for IoU calculation
        tray_bbox_np = np.array(tray_bbox).astype(np.int32)
        contour_box_np = np.array(contour_box).astype(np.int32)

        # Calculate IoU and annotate if necessary
        if intersection_over_union(tray_bbox_np, contour_box_np) > 0.5:
            label = f"Rotation Angle: {angle:.2f} degrees"
            cv.drawContours(frame, [box], 0, (0, 0, 255), 2)
            cv.putText(frame, label, (center[0] - 50, center[1]), cv.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)

            object_orientations.append((center, width, height, angle))

    if len(object_orientations) > 0:
        return object_orientations[0]
    return None

# Load YOLOv8 model
model = YOLO("yolo_models/best_4.pt")

# Open webcam
camera_stream = 0
cap = cv.VideoCapture(camera_stream)

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    # Run object detection on the frame using YOLOv8
    results = model(frame)

    # Annotate the frame with bounding boxes
    annotated_frame = results[0].plot()

    # Check if any objects of class 2 (test tubes) are detected
    detected_indices = (results[0].boxes.cls == 2).nonzero(as_tuple=True)[0]
    if detected_indices.shape[0] > 0:
        tray_bbox = results[0].boxes.xyxy[detected_indices].cpu().numpy()

        # Process the detected test tube and calculate orientation
        orientation_info = get_and_display_object_orientations(frame, tray_bbox[0])
        if orientation_info is not None:
            center, width, height, angle = orientation_info

    # Display the annotated frame
    cv.imshow('YOLOv8 Object Detection', annotated_frame)

    if cv.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv.destroyAllWindows()
